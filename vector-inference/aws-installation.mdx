---
title: 'AWS Installation'
description: 'Install Trieve Vector Inference in your own AWS account'
icon: 'aws'
mode: wide
---

<Note>This guide takes approximately 30 minutes to complete. Expect about 20 minutes of this time to be EKS spinning up.</Note>

## Installation Requirements:

- `eksctl` >= 0.171 ([eksctl installation guide](https://eksctl.io/installation))
- `aws` >= 2.15 ([AWS installation guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html))
- `kubectl` >= 1.28 ([kubectl installation guide](https://kubernetes.io/docs/tasks/tools/#kubectl))
- `helm` >= 3.14 ([Helm installation guide](https://helm.sh/docs/intro/install/#helm))
- A Trieve Vector Inference License

<Accordion title="IAM Policy Minimum Requirements">
  You need to have an IAM policy that allows you to use the `eksctl` CLI.

  The most up-to-date guide is located [here](https://eksctl.io/usage/minimum-iam-policies).

  While you can use the root account, AWS does not recommend this practice.
</Accordion>

### Getting your license

Contact us through one of these channels:
- Email: humans@trieve.ai
- [Book a meeting](https://cal.com/nick.k/meet)
- Phone: 628-222-4090
- AWS Marketplace Subscription

Our pricing details are available [here](/vector-inference/pricing).

## Check AWS Quota

<Warning>
    Ensure you have sufficient quotas for both GPUs and load balancers.
</Warning>

1) At least **4 vCPUs** for On-Demand G and VT instances in your chosen region.

Check your quota [here](https://us-west-1.console.aws.amazon.com/servicequotas/home/services/ec2/quotas/L-3819A6DF).

2) You will need **1 load balancer** for **each** model you want to deploy.

Check your load balancer quota [here](https://us-east-2.console.aws.amazon.com/servicequotas/home/services/elasticloadbalancing/quotas/L-53DA6B97).

## Deploying the Cluster

### Setting up environment variables 

Set your AWS Account ID:
```sh
export AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query "Account" --output text)"
```

Set your AWS Region:
```sh
export AWS_REGION=us-east-2
```

Set your Kubernetes cluster name:
```sh
export CLUSTER_NAME=trieve-gpu
```

Set your machine types. We recommend `g4dn.xlarge` as it is the most cost-effective option on AWS. A single small node is needed for extra utility:
```sh
export CPU_INSTANCE_TYPE=t3.small
export GPU_INSTANCE_TYPE=g4dn.xlarge
export GPU_COUNT=1
```

Optionally disable AWS CLI pagination:
```sh
export AWS_PAGER=""
```

**To use our recommended defaults:**
```sh
export AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query "Account" --output text)"
export AWS_REGION=us-east-2
export CLUSTER_NAME=trieve-gpu
export CPU_INSTANCE_TYPE=t3.small
export GPU_INSTANCE_TYPE=g4dn.xlarge
export GPU_COUNT=1
export AWS_PAGER=""
```

<Note>Trieve Vector Inference supports all regions that offer your chosen `GPU_INSTANCE` type.</Note>

### Create your cluster

Create an EKS cluster and install the required plugins.

The `bootstrap-eks.sh` script will create the EKS cluster, install the AWS Load Balancer Controller, and install the NVIDIA Device Plugin. This script also manages any IAM permissions needed for the plugins to work properly.

First, download the `bootstrap-eks.sh` script:
```sh
wget cdn.trieve.ai/bootstrap-eks.sh
```

Then run the script with bash:
```sh
bash bootstrap-eks.sh
```

This process will take approximately 25 minutes to complete.

## Install Trieve Vector Inference

### Configure `embedding_models.yaml`

First, download the example configuration file:
```sh
wget https://cdn.trieve.ai/embedding_models.yaml
```

Now you can modify your `embedding_models.yaml`. This file defines all the models that you want to use:

```yaml embedding_models.yaml
models:
  # ...
  # myEmbeddingModel:
  #   # The number of replicas you want
  #   replicas: 1
  #   # The Hugging Face revision
  #   revision: main
  #   # Your Hugging Face token if you have a private repo
  #   hfToken:
  #   # The end of the URL https://huggingface.co/BAAI/bge-m3 
  #   modelName: BAAI/bge-m3 
  bgeM3:
    replicas: 2
    revision: main
    # The end of the URL https://huggingface.co/BAAI/bge-m3
    modelName: BAAI/bge-m3 
    # If you have a private Hugging Face repo
    hfToken: "" 
  spladeDoc:
    replicas: 2
    # The end of the URL https://huggingface.co/naver/efficient-splade-VI-BT-large-doc
    modelName: naver/efficient-splade-VI-BT-large-doc 
    isSplade: true
  spladeQuery:
    replicas: 2
    # The end of the URL https://huggingface.co/naver/efficient-splade-VI-BT-large-doc
    modelName: naver/efficient-splade-VI-BT-large-doc 
    isSplade: true
  bge-reranker:
    replicas: 2
    modelName: BAAI/bge-reranker-large
    isSplade: false
  # ...
```

### Install the Helm chart

<Info>
    This Helm chart will only work if you subscribe to the AWS Marketplace listing.
</Info>
<Info>
    Contact us at humans@trieve.ai if you do not have access to the AWS Marketplace or cannot use AWS Marketplace.
</Info>

<Steps>
<Step title="Login to AWS ECR repository">
```sh
aws ecr get-login-password \
    --region us-east-1 | helm registry login \
    --username AWS \
    --password-stdin 709825985650.dkr.ecr.us-east-1.amazonaws.com
```
</Step>

<Step title="Install the Helm chart from the Marketplace ECR repository">
```sh
helm upgrade -i vector-inference \
    oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/trieve/trieve-embeddings \
    -f embedding_models.yaml
```
</Step>
</Steps>

### Get your model endpoints

Run:
```sh
kubectl get ingress
```

The output should look something like this:
```
NAME                                              CLASS   HOSTS   ADDRESS                                                                  PORTS   AGE
vector-inference-embedding-bge-reranker-ingress   alb     *       k8s-default-vectorin-18b7ade77a-2040086997.us-east-2.elb.amazonaws.com   80      73s
vector-inference-embedding-bgem3-ingress          alb     *       k8s-default-vectorin-25e84e25f0-1362792264.us-east-2.elb.amazonaws.com   80      73s
vector-inference-embedding-spladedoc-ingress      alb     *       k8s-default-vectorin-8af81ad2bd-192706382.us-east-2.elb.amazonaws.com    80      72s
vector-inference-embedding-spladequery-ingress    alb     *       k8s-default-vectorin-10404abaee-1617952667.us-east-2.elb.amazonaws.com   80      3m20s
```

The `Address` field is the endpoint where you can make [dense embeddings](/vector-inference/embed), [sparse embeddings](/vector-inference/embed_sparse), or [reranker calls](/vector-inference/reranker) based on your chosen models.

## Test your installation

To ensure everything is working, make a request to your model endpoint:

```sh
# Replace the endpoint with the one you got from the previous step
export ENDPOINT=k8s-default-vectorin-18b7ade77a-2040086997.us-east-2.elb.amazonaws.com

curl -X POST \
     -H "Content-Type: application/json"\
     -d '{"inputs": "test input"}' \
     --url "http://$ENDPOINT/embed" \
     -w "\n\nInference Took %{time_total} seconds!\n"
```

The output should look something like this:
```sh
# The vector
[[ 0.038483415, -0.00076982786, -0.020039458 ... ], [ 0.04496114, -0.039057795, -0.022400795, ... ]]
Inference Took 0.067066 seconds!
```

## Using Trieve Vector Inference

Each `ingress` point will use its own Application Load Balancer within AWS. The `Address` provided is the model's endpoint where you can make [dense embeddings](/vector-inference/embed), [sparse embeddings](/vector-inference/embed_sparse), or [reranker calls](/vector-inference/reranker) based on your chosen models.

Check out our guides for more information on configuration:

<CardGroup cols={3}>
  <Card icon="magnifying-glass" title="Using SPLADE Models" href="/vector-inference/splade">
    Learn how to set up a dedicated instance for the sparse SPLADE embedding model
  </Card>
  <Card icon="brackets-curly" title="Using Custom Models" href="/vector-inference/dense">
    Learn how to use private, gated Hugging Face models, or any models you want
  </Card>
  <Card icon="microchip-ai" title="OpenAI compatibility" href="/vector-inference/openai">
    Discover Trieve Vector Inference's OpenAI-compatible routes
  </Card>
</CardGroup>

## Optional: Delete the cluster

If you need to delete your cluster, run:

```sh
CLUSTER_NAME=trieve-gpu
REGION=us-east-2

aws eks update-kubeconfig --region ${REGION} --name ${CLUSTER_NAME}

helm uninstall vector-release
helm uninstall nvdp -n kube-system
helm uninstall aws-load-balancer-controller -n kube-system
eksctl delete cluster --region=${REGION} --name=${CLUSTER_NAME}
```