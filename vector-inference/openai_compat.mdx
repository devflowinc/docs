---
title: 'OpenAI compatible embeddings route'
sidebarTitle: 'POST /v1/embeddings'
description: 'OpenAI compatible route. Returns a 424 status code if the model is not an embedding model'
---

Generating an embedding from a dense embedding model

<RequestExample>

```json Raw JSON
{
  "encoding_format": "float",
  "input": "string",
  "model": "null",
  "user": "null"
}
```

```sh curl
curl -X POST \
     -H "Content-Type: application/json"\
     -d '{"inputs": "test input"}' \
     --url http://<your-url>/v1/embeddings 
```

```py python
import requests

endpoint = "<your-custom-endpoint>"

requests.post(f"{endpoint}/v1/embeddings", json={
    "inputs": ["test input", "test input 2"]
});

## or 

requests.post(f"{endpoint}/v1/embeddings", json={
    "inputs": "test single input"
});


```

</RequestExample>

<ResponseExample>
```json 200 Embeddings
{
  "data": [
    {
      "embedding": [
        0.038483415,
        -0.00076982786,
        -0.020039458
        ...
      ],
      "index": 0,
      "object": "embedding"
    },
    {
      "embedding": [
        0.038483415,
        -0.00076982786,
        -0.020039458
        ...
      ],
      "index": 1,
      "object": "embedding"
    },
    ...
  ],
  "model": "thenlper/gte-base",
  "object": "list",
  "usage": {
    "prompt_tokens": 512,
    "total_tokens": 512
  }
}
```

```json 413
{
    "error": "Batch size error",
    "error_type": "validation"
}
```

```json 422
{
    "error": "Tokenization error",
    "error_type": "validation"
}
```

```json 424
{
    "error": "Inference failed",
    "error_type": "backend"
}
```

```json 429
{
    "error": "Model is overloaded",
    "error_type": "overloaded"
}
```
</ResponseExample>



<ParamField path="inputs" type="string | string[]" required>
  Inputs that need to be embedded
</ParamField>

<ParamField path="normalize" type="boolean" default="true">
</ParamField>

<ParamField path="prompt_name" type="string">
The name of the prompt that should be used by for encoding. If not set, no prompt will be applied.

Must be a key in the `sentence-transformers` configuration prompts dictionary.

For example if `prompt_name` is **"doc"** then the sentence **"How to get fast inference?"** will be encoded as **"doc: How to get fast inference?"** because the prompt text will be prepended before any text to encode.
</ParamField>

<ParamField path="truncate" type="boolean" default="false">
Automatically truncate inputs that are longer than the maximum supported size
</ParamField>

<ParamField path="truncate_direction" type='"right" | "left"' default="right">
</ParamField>

